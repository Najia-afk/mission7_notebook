{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mission 7: Credit Scoring Model Implementation\n",
                "\n",
                "## Objective\n",
                "Develop a credit scoring model to predict the probability of client bankruptcy. The model must optimize for a business cost metric where False Negatives are penalized 10 times more than False Positives.\n",
                "\n",
                "## Workflow\n",
                "1. **Data Exploration (SQL)**: Load data into SQLite and perform initial exploration.\n",
                "2. **Feature Engineering**: Create domain-specific features.\n",
                "3. **Feature Analysis**: Analyze distributions and outliers.\n",
                "4. **Preprocessing**: Handle missing values, encoding, and scaling.\n",
                "5. **Model Strategy**: Define the business cost function.\n",
                "6. **Baseline Model**: Establish a baseline performance.\n",
                "7. **Model Training & Tuning**: Train models using GridSearchCV and MLflow.\n",
                "8. **Model Evaluation**: Evaluate models on the test set.\n",
                "9. **Feature Importance**: Analyze global and local feature importance.\n",
                "10. **Model Registration**: Register the best model in MLflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 0: Imports and Setup\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add src to path (works in Docker: /app/src, local: ../src)\n",
                "if os.path.exists('/app/src'):\n",
                "    sys.path.insert(0, '/app/src')\n",
                "    DATA_PATH = '/app/dataset'\n",
                "else:\n",
                "    sys.path.insert(0, os.path.abspath('../src'))\n",
                "    DATA_PATH = '../dataset'\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import mlflow\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from classes.data_loader import DataLoader\n",
                "from classes.sqlite_connector import DatabaseConnection\n",
                "from classes.feature_engineering import FeatureEngineering\n",
                "from classes.business_scorer import BusinessScorer\n",
                "from classes.model_trainer import ModelTrainer\n",
                "from classes.eda_visualizer import EDAVisualizer\n",
                "\n",
                "# Configure MLflow experiment\n",
                "mlflow.set_tracking_uri(\"http://mlflow:5005\")\n",
                "mlflow.set_experiment(\"HomeCredit_DefaultRisk\")\n",
                "\n",
                "print(f\"Data path: {DATA_PATH}\")\n",
                "print(\"Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Data Exploration (SQL)\n",
                "We will load the CSV data into a SQLite database to enable SQL-based exploration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize DataLoader and create SQLite database\n",
                "loader = DataLoader(DATA_PATH)\n",
                "db_path = os.path.join(DATA_PATH, 'home_credit.db')\n",
                "\n",
                "# Create database only if it doesn't exist\n",
                "if not os.path.exists(db_path):\n",
                "    print(\"Creating SQLite database (this may take a few minutes)...\")\n",
                "    loader.create_database(db_path)\n",
                "else:\n",
                "    print(f\"Database already exists at {db_path}\")\n",
                "\n",
                "# Connect to the database\n",
                "db = DatabaseConnection(db_path)\n",
                "print(\"Tables:\", db.get_table_names())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example SQL Query: Check target distribution in application_train\n",
                "query_target = \"\"\"\n",
                "SELECT TARGET, COUNT(*) as count \n",
                "FROM application_train \n",
                "GROUP BY TARGET\n",
                "\"\"\"\n",
                "df_target = db.execute_query(query_target)\n",
                "print(df_target)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load full training data for further processing\n",
                "df_train = db.read_table('application_train')\n",
                "print(f\"Loaded training data shape: {df_train.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Feature Engineering\n",
                "We will create new features based on domain knowledge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fe = FeatureEngineering()\n",
                "df_train = fe.simple_feature_engineering(df_train)\n",
                "print(\"Feature engineering complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Feature Analysis\n",
                "Visualize distributions and identify outliers using Plotly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target Distribution\n",
                "EDAVisualizer.plot_target_distribution(df_train).show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Numerical Distributions\n",
                "numeric_cols = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
                "EDAVisualizer.plot_numerical_distribution(df_train, columns=numeric_cols).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Preprocessing\n",
                "Prepare data for modeling: imputation, encoding, and scaling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define features\n",
                "numeric_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
                "categorical_features = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
                "\n",
                "X = df_train[numeric_features + categorical_features]\n",
                "y = df_train['TARGET']\n",
                "\n",
                "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
                "\n",
                "# Create preprocessor\n",
                "preprocessor = fe.create_preprocessor(numeric_features, categorical_features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Model Strategy\n",
                "Define the business cost function: Cost = 10 * FN + 1 * FP."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "business_scorer = BusinessScorer(fn_cost=10, fp_cost=1)\n",
                "scorer = business_scorer.get_scorer()\n",
                "print(\"Business scorer created (FN cost=10, FP cost=1)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Baseline Model\n",
                "Train a simple Logistic Regression model as a baseline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Use sample for faster training (adjust as needed)\n",
                "SAMPLE_SIZE = 50000\n",
                "X_sample = X.sample(n=SAMPLE_SIZE, random_state=42)\n",
                "y_sample = y.loc[X_sample.index]\n",
                "print(f\"Using sample of {SAMPLE_SIZE} rows for training\")\n",
                "\n",
                "pipeline_baseline = ImbPipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('smote', SMOTE(random_state=42)),\n",
                "    ('classifier', LogisticRegression(max_iter=1000))\n",
                "])\n",
                "\n",
                "param_grid_baseline = {'classifier__C': [1.0]}\n",
                "\n",
                "trainer = ModelTrainer()\n",
                "baseline_model = trainer.train_and_log(\n",
                "    pipeline_baseline, param_grid_baseline, X_sample, y_sample, scorer, \n",
                "    run_name=\"Step6_Baseline_LogReg\"\n",
                ")\n",
                "print(\"Baseline model training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Model Training & Tuning\n",
                "Train and tune more complex models (e.g., LightGBM) using GridSearchCV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lightgbm import LGBMClassifier\n",
                "\n",
                "pipeline_lgbm = ImbPipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('smote', SMOTE(random_state=42)),\n",
                "    ('classifier', LGBMClassifier(random_state=42, verbose=-1))\n",
                "])\n",
                "\n",
                "param_grid_lgbm = {\n",
                "    'classifier__n_estimators': [100, 200],\n",
                "    'classifier__learning_rate': [0.01, 0.1],\n",
                "    'classifier__num_leaves': [31, 50]\n",
                "}\n",
                "\n",
                "lgbm_model = trainer.train_and_log(\n",
                "    pipeline_lgbm, param_grid_lgbm, X_sample, y_sample, scorer, \n",
                "    run_name=\"Step7_LGBM_Tuning\"\n",
                ")\n",
                "print(\"LightGBM model training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Model Evaluation\n",
                "Evaluate the best model on the test set (if available with labels) or using cross-validation results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "# Evaluate on training sample\n",
                "y_pred = lgbm_model.predict(X_sample)\n",
                "\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_sample, y_pred))\n",
                "\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(confusion_matrix(y_sample, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Feature Importance\n",
                "Analyze feature importance for the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global Feature Importance (LightGBM built-in)\n",
                "import lightgbm as lgb\n",
                "\n",
                "lgb.plot_importance(lgbm_model.named_steps['classifier'], max_num_features=20)\n",
                "plt.title(\"Feature Importance\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Model Registration\n",
                "Register the best model in MLflow Model Registry."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Register the model (this would typically be done via MLflow UI or API)\n",
                "print(\"Please register the best model via the MLflow UI at http://localhost:5005\")\n",
                "print(\"\\nâœ… Notebook execution complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}